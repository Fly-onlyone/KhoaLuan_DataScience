{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T03:31:32.865255Z",
     "start_time": "2025-05-17T03:31:22.602159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import chardet\n",
    "import joblib\n",
    "import mrmr  # Pure-Python mRMR selection :contentReference[oaicite:1]{index=1}\n",
    "import nlpaug.augmenter.word as naw  # Contextual augmentation :contentReference[oaicite:0]{index=0}\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "id": "d51cc197aaf88f11",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T03:31:32.871932Z",
     "start_time": "2025-05-17T03:31:32.868784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ],
   "id": "17839c77692644aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:39:17.573403Z",
     "start_time": "2025-05-17T03:31:33.062977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Robust File Loader (encoding fallbacks)\n",
    "# =============================================================================\n",
    "def read_text_file(path: str) -> str:\n",
    "    \"\"\"Try multiple encodings, then chardet, always return str.\"\"\"\n",
    "    for enc in ('utf-8', 'utf-8-sig', 'latin-1', 'cp1252'):\n",
    "        try:\n",
    "            with open(path, 'r', encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    raw = open(path, 'rb').read()\n",
    "    guess = chardet.detect(raw)\n",
    "    enc = guess.get('encoding') or 'latin-1'\n",
    "    try:\n",
    "        return raw.decode(enc, errors='replace')\n",
    "    except (LookupError, UnicodeDecodeError):\n",
    "        return raw.decode('latin-1', errors='replace')\n",
    "\n",
    "\n",
    "# Load the eight-topic Vietnamese corpus\n",
    "base_dir = './Train_Full'\n",
    "records = []\n",
    "for topic in os.listdir(base_dir):\n",
    "    topic_path = os.path.join(base_dir, topic)\n",
    "    if os.path.isdir(topic_path):\n",
    "        for fname in os.listdir(topic_path):\n",
    "            if fname.lower().endswith('.txt'):\n",
    "                txt = read_text_file(os.path.join(topic_path, fname))\n",
    "                if txt:\n",
    "                    records.append({'text': txt, 'label': topic})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(f\"Loaded {len(df)} documents across {df['label'].nunique()} topics.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Preprocessing & Tokenization (named functions)\n",
    "# =============================================================================\n",
    "# Load Vietnamese stopwords\n",
    "with open('vietnamese-stopwords.txt', encoding='utf-8') as f:\n",
    "    VI_STOPWORDS = set(w.strip() for w in f if w.strip())\n",
    "\n",
    "# Regexes for noise removal\n",
    "EMOJI_RE    = re.compile(\"[\\U0001F600-\\U0001F64F\"\n",
    "                         \"\\U0001F300-\\U0001F5FF\"\n",
    "                         \"\\U0001F680-\\U0001F6FF\"\n",
    "                         \"\\U0001F1E0-\\U0001F1FF]+\", flags=re.UNICODE)\n",
    "REPEAT_PUNC = re.compile(r'([!?.,])\\1+')\n",
    "CONTROL_RE  = re.compile(r'[\\x00-\\x1F\\x7F]+')\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Unicode normalize, lowercase, strip URLs/emails/digits/punc, remove emojis/control.\"\"\"\n",
    "    text = unicodedata.normalize('NFC', text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+|\\S+@\\S+|\\d+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = EMOJI_RE.sub(' ', text)\n",
    "    text = REPEAT_PUNC.sub(r'\\1', text)\n",
    "    text = CONTROL_RE.sub(' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split on whitespace and remove Vietnamese stopwords.\"\"\"\n",
    "    toks = text.split()\n",
    "    return [t for t in toks if t not in VI_STOPWORDS]\n",
    "\n",
    "\n",
    "# Apply cleaning + tokenization\n",
    "df['clean']  = df['text'].apply(clean_text)\n",
    "df['tokens'] = df['clean'].apply(tokenize)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Encode Labels & Train/Test Split\n",
    "# =============================================================================\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label'])\n",
    "print(\"Label→ID mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "X = df['tokens']\n",
    "y = df['label_id']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Contextual Synonym Augmentation (multilingual BERT)\n",
    "# =============================================================================\n",
    "syn_aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-multilingual-cased',  # Vietnamese support :contentReference[oaicite:2]{index=2}\n",
    "    action='substitute',  # token substitution :contentReference[oaicite:3]{index=3}\n",
    "    top_k=5,              # candidate pool :contentReference[oaicite:4]{index=4}\n",
    "    aug_p=0.3,            # 30% tokens per sentence :contentReference[oaicite:5]{index=5}\n",
    "    aug_min=1,            # at least 1 token :contentReference[oaicite:6]{index=6}\n",
    "    aug_max=3,            # at most 3 tokens :contentReference[oaicite:7]{index=7}\n",
    "    device='cuda'          # or 'cuda' :contentReference[oaicite:8]{index=8}\n",
    ")\n",
    "\n",
    "aug_texts, aug_labels = [], []\n",
    "for toks, lbl in zip(X_train, y_train):\n",
    "    out = syn_aug.augment(' '.join(toks), n=1)  # returns list of strings\n",
    "    for sent in out:\n",
    "        aug_texts.append(sent.split())\n",
    "        aug_labels.append(lbl)\n",
    "\n",
    "X_train = list(X_train) + aug_texts\n",
    "y_train = list(y_train) + aug_labels\n",
    "print(\"After augmentation, train size:\", len(X_train))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 5. TF–IDF & mRMR Feature Selection\n",
    "# =============================================================================\n",
    "# 5.1 Named identity functions for vectorizer\n",
    "def identity_preprocessor(x): return x\n",
    "def identity_tokenizer(x):    return x\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=identity_preprocessor,\n",
    "    tokenizer=identity_tokenizer,\n",
    "    token_pattern=None,\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.9,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform([' '.join(t) for t in X_train])\n",
    "X_test_tfidf  = vectorizer.transform([' '.join(t) for t in X_test])\n",
    "print(\"TF–IDF features:\", X_train_tfidf.shape[1])  #\n",
    "\n",
    "# 5.2 mRMR via pure-Python mrmr_selection\n",
    "feat_df = pd.DataFrame(\n",
    "    X_train_tfidf.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "selected = mrmr.mrmr_classif(\n",
    "    X=feat_df,\n",
    "    y=pd.Series(y_train, name='target'),\n",
    "    K=2000\n",
    ")\n",
    "idx = [vectorizer.vocabulary_[f] for f in selected]\n",
    "X_train_sel = X_train_tfidf[:, idx]\n",
    "X_test_sel  = X_test_tfidf[:, idx]\n",
    "print(\"mRMR selected features:\", len(selected))  # :contentReference[oaicite:11]{index=11}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Pipeline Assembly & Training\n",
    "# =============================================================================\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        preprocessor=clean_text,\n",
    "        tokenizer=tokenize,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1,2),\n",
    "        max_df=0.9,\n",
    "        min_df=5\n",
    "    )),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit([' '.join(t) for t in X_train], y_train)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Save Pipeline & Artifacts (no lambdas!)\n",
    "# =============================================================================\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Only save the pipeline (it contains our named functions)\n",
    "joblib.dump(pipeline, 'output/vi_text_pipeline.joblib')\n",
    "print(\"✅ Pipeline saved to output/vi_text_pipeline.joblib\")\n",
    "\n",
    "# Save selected feature names (plain list)\n",
    "joblib.dump(selected, 'output/mrmr_selected_features.joblib')\n",
    "print(\"✅ mRMR features saved to output/mrmr_selected_features.joblib\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Evaluation\n",
    "# =============================================================================\n",
    "y_pred = pipeline.predict([' '.join(t) for t in X_test])\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n"
   ],
   "id": "cd168e09d807cb61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42744 documents across 8 topics.\n",
      "Label→ID mapping: {'Chinh tri Xa hoi': np.int64(0), 'Doi song': np.int64(1), 'Kinh doanh': np.int64(2), 'Phap luat': np.int64(3), 'Suc khoe': np.int64(4), 'The gioi': np.int64(5), 'The thao': np.int64(6), 'Van hoa': np.int64(7)}\n",
      "Train size: 34195, Test size: 8549\n",
      "After augmentation, train size: 68390\n",
      "TF–IDF features: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [00:40<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mRMR selected features: 154\n",
      "✅ Pipeline saved to output/vi_text_pipeline.joblib\n",
      "✅ mRMR features saved to output/mrmr_selected_features.joblib\n",
      "Test Accuracy: 0.898935548017312\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Chinh tri Xa hoi       0.83      0.84      0.83      1314\n",
      "        Doi song       0.85      0.84      0.84       839\n",
      "      Kinh doanh       0.88      0.89      0.88       855\n",
      "       Phap luat       0.92      0.91      0.91      1331\n",
      "        Suc khoe       0.92      0.91      0.92       883\n",
      "        The gioi       0.92      0.93      0.93      1143\n",
      "        The thao       0.98      0.96      0.97      1134\n",
      "         Van hoa       0.89      0.90      0.90      1050\n",
      "\n",
      "        accuracy                           0.90      8549\n",
      "       macro avg       0.90      0.90      0.90      8549\n",
      "    weighted avg       0.90      0.90      0.90      8549\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
